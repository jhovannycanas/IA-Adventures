{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras practica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El principal bloque de construcción de keras es un modelo, llamado secuencial, el cual es un proceso lineal (una pila) de capa de redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente fragmento de código define una sola capa con 12 neuronas y espera 8 variables de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='random_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primer ejemplo crearemos una red neuronal que aprenda a dividir dos grupos de conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=1000, centers=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la distribucion de los conjuntos podemos trazar una linea que divida mejor los conjunots, problema que ya repasamos con SVM.Para conocer cual es la mejor linea que divide estos dos conjuntos implementaremos un red neuronal en keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El patron general para crear una red neuronal con keras es el siguiente:\n",
    "- crear el modelo\n",
    "- agregar capas\n",
    "- compilar el modelo\n",
    "- entrenar el modelo\n",
    "- evaluar el desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Simple Sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red es densa, lo que significa que cada neurona en una capa está conectada a todas las neuronas localizadas en la capa anterior y a todas las neuronas en la capa siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos una capa Densa con una neurona.Luego configuramos input_shape=(2,), que significa que la entrada de datos corresponde a una matriz de la forma (*,2). La primera dimensión será una dimensión no especificada \n",
    "número de lotes (filas) de datos.  La segunda dimensión es 2 que son las posiciones X, Y de cada elemento de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de activación sigmoide se utiliza para devolver 0 o 1, lo que significa que los datos del conjunto se utliza para determinar su prediccion en las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para aprender más sobre las funciones de activación visitar el siguiente enlace: https://keras.io/activations/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, input_shape=(2,), activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cada neurona es inicializada con un especifico peso;<br>\n",
    "- random_uniform: Weights are initialized to uniformly random small values in (-0.05, 0.05). In other words, any value within the given interval is equally likely to be drawn.\n",
    "- random_normal: Weights are initialized according to a Gaussian, with a zero mean and small standard deviation of 0.05. For those of you who are not familiar with a Gaussian, think about a symmetric bell curve shape.\n",
    "- zero: All weights are initialized to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para revisar mas sobre este apartado visitar: https://keras.io/initializations/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos el modelo, minimizando la crossentopy para respuesta binarias y maximizando la  precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El optimizador Adam realizara una propagacion hacia atras para ajustar los pesos y los sesgos para minimizar el error durante la fase de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.05), 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas funciones de perdida comunes son las siguientes: visitar la página para mayor información:https://keras.io/objectives/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSE: Estas funciones objetivas promedian todos los errores cometidos para cada predicción, y si la predicción está lejos del valor real, entonces esta distancia se hace más evidente por la operación de cuadrado.\n",
    "- Binary cross-entropy: Esta función de objetivo es adecuada para la predicción de etiquetas binarias.\n",
    "- Categorical cross-entropy:  Esta función de objetivo es adecuada para la predicción de etiquetas multiclase. También es la opción predeterminada en asociación con la activación de softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas opciones comunes para las métricas (una lista completa de las métricas de Keras se encuentra en https://keras.io/metrics/) son las siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: This is the proportion of correct predictions with respect to the targets\n",
    "- Precision: This denotes how many selected items are relevant for a multilabel classification\n",
    "- Recall: This denotes how many selected items are relevant for a multilabel classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las métricas son similares a las funciones objetivas, con la única diferencia de que no se utilizan para entrenar un modelo sino sólo para evaluar un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo con los datos. Determinamos que realice 100 ciclos a traves de los datos. Configurar verbose = 1 para mostrar un mensaje de progreso de la operacion, de otra menra colocar cero para ocultarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "épocas: Este es el número de veces que el modelo está expuesto al set de entrenamiento. En cada iteración, el optimizador intenta ajustar los pesos para minimizar la función objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Obtener la perdida y la precisión en los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imprimir la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTest loss:\", eval_result[0], \"Test accuracy:\", eval_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora trabajaremos con unos datos con otro tipo de distribucion diferente a la anterior que era de tipo lineal, para observar el comportamiento de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "x, y = make_circles(n_samples=1000, factor=.6, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=100, verbose=1)\n",
    "eval_result = model.evaluate(x_test, y_test)\n",
    "print(\"\\n\\nTest loss:\", eval_result[0], \"Test accuracy:\", eval_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar los resultados, se hace necesario crear un modelo mas potente con mas capas y con mayor numero de neuronas, a continuación procedemos a crear mas capas y mas neuronas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuantas capas y neuronas en cada capa son requeridas para nuestra red neuronal, para responder esta pregunta no hay un criterio exacto, pero tenemos una pautas aproximadas que se puede estudiar en el siguiente articulo: https://www.heatonresearch.com/2017/06/01/hidden-layers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiLayer = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiLayer.add(Dense(4,input_shape = (2,), activation = \"tanh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiLayer.add(Dense(4,activation = \"tanh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiLayer.add(Dense(1,activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiLayer.compile(Adam(lr=0.05), 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMultiLayer.fit(x_train, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = modelMultiLayer.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTest loss:\", eval_result[0], \"Test accuracy:\", eval_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora pasaremos a un ejemplo con datos reales, mediante la construcción de una red neural que pueda reconocer manuscritos de numeros, para lo cual utilizaremos el dataset de MNIST disponible en el siguiente enlace: http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras proporciona una libreria con un conjunto de datos, entre ellos el MNIST, los datos se encuentran tratados y normalizados de [0,1] para trabajar directamente con ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de aprendizaje profundo esperan que todas las características de entrada varíen de manera similar, e idealmente que tengan una media de 0, y una varianza de 1, es decir los datos se encuentren normalizados. Debemos reescalar nuestros datos para que cumplan con estos requisitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1671) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red y entrenamiento\n",
    "N_EPOCAS = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "N_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # SGD optimizer, explained later in this chapter\n",
    "N_CAPAS_OCULTAS = 128\n",
    "VALIDATION_TEST=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "RESHAPED = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784RESHAPED = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir de variable categorica a numerica, onehotencoding\n",
    "Y_train = np_utils.to_categorical(y_train, N_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMnist = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMnist.add(Dense(N_CLASSES, input_shape=(RESHAPED,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMnist.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMnist.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMnist.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMnist.fit(X_train, Y_train,batch_size=BATCH_SIZE, epochs=N_EPOCAS, verbose=VERBOSE, validation_split=VALIDATION_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelMnist.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejorar la red coN capas ocultas, en este caso haremos una pequeña variación con solo 20 epocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCAS = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_CAPAS_OCULTAS, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_CAPAS_OCULTAS))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=N_EPOCAS, verbose=VERBOSE, validation_split=VALIDATION_TEST)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, disminuyendo las epocas y con una capa oculta obtenemos una mejora considerable en el rendimiento y mejora de 2 puntos en la precisión del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales tiendes a sufrir de sobreajuste con pocos datos, lo cual imposibilita su generalización. Para abordar este inconveniente podemos utilizar la tecnica Dropout para regularizar los modelos de aprendizaje profundo. Explicaciones en detalle de esta tecnica se encuentran disponible en los siguientes enlaces: https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/, y la implementacion en keras: https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/, a continuación procedemos a configurar dropout a nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dropout\n",
    "DROPOUT = 0.3\n",
    "N_EPOCAS = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_CAPAS_OCULTAS, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_CAPAS_OCULTAS))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=N_EPOCAS, verbose=VERBOSE, validation_split=VALIDATION_TEST)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si es necesario, puede configurar aún más su optimizador. Un principio básico de Keras es hacer las cosas razonablemente simples, mientras que permite al usuario tener el control total cuando lo necesite (el control final es la fácil extensibilidad del código fuente). Aquí utilizamos SGD (Estocástico de descenso en gradiente) como algoritmo de optimización para nuestras pesas entrenables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![saddle_point_evaluation_optimizers](img/saddle_point_evaluation_optimizers.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias y agradecimientos especiales:\n",
    "- Gully A, Pal S. Deep Learning with Keras : Implement various deep-learning algorithms in Keras and see how deep-learning can be used in games.\n",
    "- Raschka S. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow.\n",
    "- Jerry Kurata: https://twitter.com/jerrykur?lang=es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
