{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales convolucionales (también llamadas ConvNet) aprovechan la información espacial y, por lo tanto, son muy adecuadas para clasificar imágenes.<br><br>\n",
    "Las redes neuronales convolucionales consisten en múltiples capas de pequeñas colecciones de neuronas que procesan porciones de la imagen de entrada, llamadas campos receptivos.<br><br>\n",
    "Estas redes utilizan una arquitectura ad hoc inspirada en datos biológicos extraídos de experimentos fisiológicos realizados en la corteza visual (científicos estudiaron el cerebro de los gatos para reconocer las aves).<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La visión humana tiene múltiples niveles de corteza, donde cada uno de ellos reconoce más y más información estructurada. Primero, vemos pixeles individuales; luego, a partir de ellos, reconocemos formas geométricas simples. Y luego … elementos mas sofisticados como objetos, rostros, cuerpos humanos, animales, etc. La visión humana no reconoce una imagen como un todo, en cambio, las neuronas del cerebro perciben la imagen como una división pequeña de partes y cada neurona del ojo tiene su propio campo receptivo local, es decir solo recolecta los datos que se encuentra en ese campo receptivo. Es decir, una sola neurona en el ojo reacciona a los estímulos visuales que se ubiquen en su campo receptivo. Pero el cerebro no consta de una sola neurona, por lo que otras neuronas reaccionan a patrones mas complejos, estas neuronas que reaccionan a patrones mas complejos reciben su entrada de otras neuronas de nivel inferior, así que combinan estas entradas para extraer información de mayor complejidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales convolucionales son sorprendentes. Desde su irrupción en se convirtieron en una tecnología disruptiva, cambiando el estado del arte en múltiples dominios como el análisis de texto, luego pasando por el video, el habla, abarcando mucho más del procesamiento de imágenes inicial en el que fue concebido originalmente este tipo de red.<br>\n",
    "<br>\n",
    "Su nacimiento de debe principalmente para mejorar la eficiencia y rendimiento para tareas de clasificación, esto dado principalmente por el defecto de las redes neuronales profundas cuando se utilizan en tareas de clasificación con imágenes la explosión de parámetros, en una red densa, cada neura debe estar conectada con las todas las neuronas de la capa siguiente, supongamos una imagen de  120 *120, es decir que la imagen esta conformada por 14.400 pixeles, entonces la primera capa tiene 14400 neuronas, luego cada neurona de estar conectada con un numero igual de neuronas de la capa posterior para procesar esta imagen  lo que daría: 207.360.000 parámetros a entrenar en la red neuronal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que es Convolución?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolución es una función matemática de ventana deslizante que se aplica a una matriz, la función que se aplica se denomina filtro o nucleo (kernel) , esta función imita el campo receptivo de las neuronas de nivel inferior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtros<br>\n",
    "Modifican matemáticamente la entrada de una convolución para ayudarlo a detectar ciertas características en la imagen. Pueden devolver la imagen sin modificar, desenfocar, mejorar la imagen, detectar bordes y más, esto se hace multiplicando los valores de la imagen original por una matriz de convolución como las cuatro matrices que se muestran, estos filtros ayudan que la CNN detecte ciertas características de una imagen haciendo transformaciones. <br>\n",
    "\n",
    "La agrupación(pooling), es el siguiente paso en una red neuronal convolucional. Agrupación reduce el número de neuronas en la capa convolucional anterior, al tiempo que conserva la información más importante. Hay diferentes tipos de agrupación que se pueden realizar. El promedio, la suma de los valores el máximo valor de la neurona.<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling (agrupación o puesta en común), implica sub-muestrear las entradas para extraer patrones y abstracciones de un nivel superior. Una opción fácil y común es el max-pooling, que simplemente produce la activación máxima observada en la región, mediante la aplicación de un filtro de agrupación de 2 x 2, que da como resultado una sub matriz de 2 x 2. Es decir lo que hace es el kernel es considerar las 4 primeras celda de la matriz de entrada, luego elige el valor máximo de esas entradas y ese valor pasa hacer parte de la matriz de resultados 2 x 2 que definimos en el filtro de agrupación, luego continua aplicando el kernel a las demás agrupaciones de celdas de entrada. Otros kernels utilizados son la suma y el promedio.<br><br>\n",
    "Estas capas de agregación permiten a las redes convolucionales extraer abstracciones de alto nivel, también permite reducir enormemente el uso de memoria durante el entrenamiento, al submuestrar la entrada. También al estar expuesta a miles de parámetros son muy propensas a sobreajustes en la fase de entrenamiento, pero mediante las capas de agrupación solo se extrae la información relevante o importante, esto es submuestreo y mitiga el sobre ajuste.<br>\n",
    "También le permiten a reconocer características independientes de la posición, lo cual se denomina invarianza de ubicación, es decir nos permite identificar un objeto independiente de su ubicación ya sea en la parte superior, izquierda, etc, caso diferente en las redes neuronales densas que no cuentan con esta característica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura de una CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red neuronal convolucional profunda (DCNN) consiste en muchas capas de red neuronal  a las cuales se le pasa una imagen. Normalmente se alterna dos tipos diferentes de capas, la convolucional y la de acumulación. La profundidad de cada filtro aumenta de izquierda a derecha en la red. La última etapa se compone típicamente de una o más capas completamente conectadas. para finalmente obtener una salida que puede ser una clase o un conjunto de probabilidades que se ajustan en mayor grado a la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera capa de una CNN es una capa convolucional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imagen aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, podemos codificar más información teniendo submatrices superpuestas. Por ejemplo, supongamos que el tamaño de cada submatriz es de 5 x 5 y que esas submatrices se utilizan con imágenes MNIST de 28 x 28 píxeles. Entonces podremos generar 23 x 23 neuronas de campo receptivas locales en la siguiente capa oculta. De hecho, es posible deslizar las submatrices en sólo 23 posiciones antes de tocar los bordes de las imágenes. En Keras, el tamaño de cada submatriz se llama longitud de zancada, y este es un hiperparámetro que puede ser afinado durante la construcción de nuestras redes.<br><br>\n",
    "Vamos a definir el mapa de características de una capa a otra. Por supuesto, podemos tener múltiples mapas de características que aprenden independientemente de cada capa oculta. Por ejemplo, podemos empezar con 28 x 28 neuronas de entrada para procesar imágenes MINST y luego recordar k mapas de características de tamaño 23 x 23 neuronas cada uno (de nuevo con una zancada de 5 x 5) en la siguiente capa oculta.<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos alejarnos de la representación de píxeles en una fila, obteniendo la capacidad de detectar la misma característica independientemente de la ubicación en la que se encuentra en la imagen de entrada. Una simple intuición es usar el mismo conjunto de pesos y sesgos para todas las neuronas en las capas ocultas. De esta manera, cada capa aprenderá un conjunto de características latentes independientes de la posición derivadas de la imagen. <br><br>\n",
    "Asumiendo que la imagen de entrada tiene forma (256, 256) en tres canales con orden tf (TensorFlow), esto se representa como (256, 256, 256, 3). Nótese que con el modo th (Theano), la dimensión del canal (la profundidad) está en el índice 1; en el modo tf (TensoFlow), está en el índice 3. En Keras, si queremos añadir una capa convolucional con dimensionalidad de la salida 32 y extensión de cada filtro 3 x 3, escribiremos: <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()<br>\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(256, 256, 3))<br><br>\n",
    "\n",
    "o <br><br>\n",
    "          \n",
    "model = Sequential()<br>\n",
    "model.add(Conv2D(32, kernel_size=3, input_shape=(256, 256, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto significa que estamos aplicando una convolución de 3 x 3 en una imagen de 256 x 256 con tres canales de entrada (o filtros de entrada), resultando en 32 canales de salida (o filtros de salida):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos resumir la salida de un mapa de características. Una vez más, podemos usar la contigüidad espacial de la salida producida a partir de un solo mapa de características y agregar los valores de una submatriz en un solo valor de salida que describa sintéticamente el significado asociado con esa región física. <br><br>\n",
    "\n",
    "Hasta ahora, hemos descrito los conceptos básicos de ConvNets. Las CNN aplican operaciones de convolución y agrupación en una dimensión para datos de audio y texto a lo largo de la dimensión temporal, en dos dimensiones para imágenes a lo largo de las dimensiones (alto x ancho), y en tres dimensiones para vídeos a lo largo de las dimensiones (alto x ancho x tiempo). Para las imágenes, al deslizar el filtro sobre el volumen de entrada se produce un mapa que muestra las respuestas del filtro para cada posición espacial. En otras palabras, un ConvNet tiene múltiples filtros apilados que aprenden a reconocer características visuales específicas independientemente de la ubicación de la imagen. Esas características visuales son simples en las capas iniciales de la red, y luego más y más sofisticadas en la red.<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
