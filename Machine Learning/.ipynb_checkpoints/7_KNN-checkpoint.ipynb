{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN o k vecinos más cercanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un método de clasificación no paramétrico, que estima el valor de la probabilidad a posteriori de que un elemento xx pertenezca a una clase en particular a partir de la información proporcionada por el conjunto de prototipos. La regresión KNN se calcula simplemente tomando el promedio del punto k más cercano al punto que se está probando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN, o k-Nearest Neighbors, es un algoritmo supervizado de clasificación y regresion. Sin embargo, difiere de los clasificadores descritos anteriormente porque es un modelo de aprendizaje perezoso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es un estudiante perezoso? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de aprendizaje perezoso no hace mucho durante el proceso de capacitación más que almacenar los datos de la capacitación. Sólo cuando se introducen nuevos datos sin etiquetar, este tipo de estudiante busca clasificar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, un modelo aprendizaje activo construye un modelo de clasificación durante la capacitación. Cuando se introducen nuevos datos no etiquetados, este tipo de estudiante introduce los datos en el modelo de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizaje perezoso: en realidad el algoritmo solo se ejecuta en el momento que se requiere predecir una nueva instancia a partir de una predicción local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, ¿qué hace KNN?<br>\n",
    "kNN no construye ningún modelo de clasificación de este tipo. En su lugar, sólo almacena los datos de formación etiquetados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sencillo: asignar la clase o valor agregado de las instancias conocidas que se encuentran mas cerca de la instancia a predecir<br>\n",
    "Basado en las instancias de aprendizaje, no en un modelo subyacente probabilístico/estadístico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando llegan nuevos datos sin etiquetar, kNN funciona en dos pasos básicos:\n",
    "En primer lugar, examina los k puntos de datos de entrenamiento más cercanos, es decir, los vecinos más cercanos.\n",
    "Segundo, usando las clases de los vecinos, kNN obtiene una mejor idea de cómo se deben clasificar los nuevos datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se da cuenta kNN de lo que está más cerca? <br>\n",
    "Para los datos continuos, kNN utiliza una métrica de distancia como la distancia euclídea. La elección de la métrica de distancia depende en gran medida de los datos. Algunos incluso sugieren aprender una métrica de distancia basada en los datos de la capacitación. Hay muchos más detalles y documentos sobre las métricas de distancia kNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso de la distancia de Hamming como métrica para la \"cercanía\" de dos cadenas de texto.\n",
    "\n",
    "Depende de la definición de una función de distancia, que se escogerá según la cantidad y características de las variables independientes\n",
    "\n",
    "\n",
    "Transformación de datos discretos en características binarias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo clasifica kNN los nuevos datos cuando los vecinos no están de acuerdo? \n",
    "kNN tiene un tiempo fácil cuando todos los vecinos son de la misma clase. La intuición es que si todos los vecinos están de acuerdo, entonces el nuevo punto de datos probablemente caiga en la misma clase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo decide kNN la clase cuando los vecinos no tienen la misma clase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 técnicas comunes para tratar con esto son:<br>\n",
    "\n",
    "Tomar una mayoría simple de votos de los vecinos. Cualquier clase que tenga el mayor número de votos se convierte en la clase para el nuevo punto de datos.\n",
    "Tomar un voto similar, excepto dar un peso mayor a los vecinos que están más cerca. Una manera sencilla de hacer esto es usar la distancia recíproca, por ejemplo, si el vecino está a 5 unidades de distancia, entonces pondere su voto 1/5. A medida que el vecino se aleja, la distancia recíproca se hace cada vez más pequeña.... ¡exactamente lo que queremos!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué usar kNN? \n",
    "La facilidad de comprensión e implementación son dos de las razones clave para usar kNN. Dependiendo de la métrica de distancia, el kNN puede ser bastante preciso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hay 5 cosas a las que hay que prestar atención:\n",
    "\n",
    "kNN puede resultar muy costoso desde el punto de vista computacional cuando se trata de determinar los vecinos más cercanos en un conjunto de datos de gran tamaño.\n",
    "Los datos ruidosos pueden desviar las clasificaciones kNN.\n",
    "Las características con un rango mayor de valores pueden dominar la métrica de distancia en relación con las características que tienen un rango menor, por lo que el escalado de características es importante.\n",
    "Dado que el procesamiento de datos es diferido, el kNN generalmente requiere mayores requerimientos de almacenamiento que los clasificadores ansiosos.\n",
    "La selección de una buena métrica de distancia es crucial para la precisión del kNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que significa el parámetro K\n",
    "\n",
    "número de vecinos mas cercanos a considerar para establecer la clase o valor de una nueva instancia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consideraciones a tener en cuenta con el parámetro K:\n",
    "\n",
    "\n",
    "K controla el overfitting(sobre aprendizaje) y el underfitting(sub aprendizaje\n",
    "•Modelos más sencillos (K mas grandes) previenen el overfitting, pero pueden por el contrario irse hacia el underfitting\n",
    "•Modelos mas complejos (K mas pequeños) previenen el underfitting, pero pueden por el contrario irse hacia el overfitting\n",
    "•El K ideal que sirva para todos los casos no existe, depende de cada datasetespecífico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las mismas consideraciones aplican tanto para la clasificación como para la regresión a partir de KNN (encontrar el K ideal para prevenir el over/underfitting)\n",
    "Ejemplo: puntos en rojo resultado de una función lineal con ruido (lo que se quiere aprender)\n",
    "•2 modelos KNN en azul (izquierda y derecha)\n",
    "•3 modelo lineal ideal en negro\n",
    "•¿En cuál de los dos modelos KNN hay overfitting)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overfiting](img/overfiting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-18e5a269e91f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Creando el dataset iris\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0miris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Creando el dataset iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "iris.feature_names\n",
    "\n",
    "# importando KNN \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knnr = KNeighborsRegressor(n_neighbors=10) # Creando el modelo con 10 vecinos\n",
    "knnr.fit(X, y) # Ajustando el modelo\n",
    "\n",
    "# Verificando el error medio del modelo\n",
    "print(\"El error medio del modelo es: {:.2f}\".format(np.power(y - knnr.predict(X),\n",
    "2).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
