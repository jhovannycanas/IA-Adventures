{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM o máquinas de vectores de soporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea detrás de SVM es encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aquí, la separación significa que la elección del plano maximiza el margen entre los puntos más cercanos en el plano; estos puntos se denominan vectores de soporte.<br>\n",
    "Las máquinas vectoriales de soporte (SVM) son un potente algoritmo de aprendizaje supervisado que se utiliza para la clasificación o para la regresión. Los SVMs son un clasificador discriminatorio: es decir, trazan un límite entre grupos de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La máquina vectorial de soporte (SVM) aprende un hiperplano para clasificar los datos en 2 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un hiperplano es una función como la ecuación de una recta, y = mx + b. De hecho, para una simple tarea de clasificación con sólo 2 características, el hiperplano puede ser una recta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM puede realizar un truco para proyectar sus datos en dimensiones superiores. Una vez que se proyecta en dimensiones superiores....\n",
    "...SVM calcula el mejor hiperplano que separa sus datos en las dos clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Tienes un ejemplo? \n",
    "Absolutamente, el ejemplo más simple que encontré comienza con un montón de bolas rojas y azules en una mesa. Si las bolas no están muy mezcladas, puedes tomar un palo y sin moverlo, separarlas con el palo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verás:\n",
    "\n",
    "Cuando se añade una nueva bola a la mesa, sabiendo en qué lado del palo está la bola, se puede predecir su color.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué representan las bolas, la mesa y el palo? \n",
    "Las bolas representan puntos de datos, y el color rojo y azul representan 2 clases. El palo representa el hiperplano que en este caso es una línea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El SVM calcula la función para el hiperavión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y si las cosas se complican? \n",
    "Claro, lo hacen con frecuencia. Si las bolas se mezclan, un palo recto no funcionará.\n",
    "\n",
    "Aquí está la solución:\n",
    "\n",
    "Levante rápidamente la mesa lanzando las bolas al aire. Mientras las bolas están en el aire y se lanzan de la manera correcta, se utiliza una hoja grande de papel para dividir las bolas en el aire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, levantar la mesa es el equivalente a mapear sus datos a dimensiones más altas. En este caso, pasamos de la superficie de la mesa de 2 dimensiones a las bolas tridimensionales en el aire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo lo hace el SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando un kernel tenemos una buena forma de operar en dimensiones superiores. La gran hoja de papel todavía se llama hiperplano, pero ahora es una función de un plano en lugar de una línea. Nota de Yuval que una vez que estamos en 3 dimensiones, el hiperplano debe ser un plano en lugar de una línea.\n",
    "Estos videos explicativos pueden ser muy útiles para reforzar el concepto de SVM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?time_continue=23&v=3liCbRZPrZA\n",
    "\n",
    "https://www.youtube.com/watch?v=1NxnPkZM9bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "¿Cómo hacen las bolas en una mesa o en el mapa aéreo para obtener datos de la vida real?<br>\n",
    "Una bola en una mesa tiene una ubicación que podemos especificar usando coordenadas. Por ejemplo, una pelota puede estar a 20 cm del borde izquierdo y a 50 cm del borde inferior. Otra manera de describir la bola es como (x, y) coordenadas o (20, 50). x e y son 2 dimensiones de la bola.<br>\n",
    "Este es el trato:<br>\n",
    "Si tuviéramos un conjunto de datos de pacientes, cada paciente podría ser descrito por varias medidas como pulso, nivel de colesterol, presión arterial, etc. Cada una de estas medidas es una dimensión.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La conclusión es que:<br>\n",
    "El SVM hace lo suyo, los mapea a una dimensión superior y luego encuentra el hiperplano para separar las clases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los márgenes están a menudo asociados con el SVM? ¿Qué son ellos?<br>\n",
    "El margen es la distancia entre el hiperplano y los 2 puntos de datos más cercanos de cada clase respectiva. En el ejemplo de bola y mesa, la distancia entre el palo y la bola roja y azul más cercana es el margen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El SVM intenta maximizar el margen, de modo que el hiperplano esté tan lejos de la bola roja como la azul. De este modo, se reduce la posibilidad de que se produzcan errores de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿De dónde viene el nombre del SVM?<br>\n",
    "Usando el ejemplo de la bola y la mesa, el hiperplano está equidistante de una bola roja y una azul. Estas bolas o puntos de datos se llaman vectores de apoyo, porque soportan el hiperplano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasemos al ejemplo donde podamos practicar el concepto:\n",
    "Primero crearemos un conjunto de datos para simular la tabla y las bolas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un clasificador discriminatorio intenta trazar una línea entre los dos conjuntos de datos. Inmediatamente vemos un problema: ¡una línea así está mal colocada! Por ejemplo, podríamos encontrar varias posibilidades que discriminan perfectamente entre las clases de este ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máquinas Vectoriales de Soporte: Maximizar el margen\n",
    "Las máquinas vectoriales de soporte son una forma de abordar este problema. Lo que el vector de soporte mecanizado hace es no sólo dibujar una línea, sino considerar una región sobre la línea de un ancho determinado. He aquí un ejemplo de cómo podría ser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note aquí que, si queremos maximizar este ancho, el ajuste medio es claramente el mejor. Esta es la intuición de las máquinas vectoriales de soporte, que optimizan un modelo discriminante lineal junto con un margen que representa la distancia perpendicular entre los conjuntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montaje de una máquina vectorial de soporte<br>\n",
    "Ahora ajustaremos un Clasificador de Vectores de Soporte a estos puntos. Aunque los detalles matemáticos del modelo de probabilidad son interesantes, le dejaremos leer sobre ellos en otra parte. En su lugar, trataremos el algoritmo scikit-learn como una caja negra que cumple la tarea anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar mejor lo que está sucediendo aquí, vamos a crear una función de conveniencia rápida que trazará los límites de las decisiones del SVM para nosotros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([[xi, yj]])\n",
    "    # plot the margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
    "plot_svc_decision_function(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que las líneas discontinuas tocan un par de puntos: estos puntos son las piezas centrales de este ajuste, y se conocen como los vectores de soporte (dando al algoritmo su nombre). En scikit-learn, estos se almacenan en el atributo support_vectors_ del clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora utilizamos un conjunto de datos de la vida real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importanto SVM\n",
    "from sklearn import svm\n",
    "\n",
    "# importando el dataset iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # solo tomamos las primeras 2 características\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # tamaño de la malla del grafico\n",
    "\n",
    "# Creando el SVM con sus diferentes métodos\n",
    "C = 1.0  # parametro de regulacion SVM \n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titulos de los graficos\n",
    "titles = ['SVC con el motor lineal',\n",
    "          'LinearSVC',\n",
    "          'SVC con el motor RBF',\n",
    "          'SVC con el motor polinomial']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "    # Realizando el gráfico, se le asigna un color a cada punto\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "    # Graficando tambien los puntos de datos\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('largo del petalo')\n",
    "    plt.ylabel('ancho del petalo')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
